AnsibleUndefinedVariable: ansible.vars.hostvars.Hostvars object has no element

    when: inventory_hostname in groups['small_group']


  gather_facts: yes

  ou 

  tasks:
    - name:
      setup:
      become: true

  ---------------------------------------------------------------------------------

FORÇAR UM GATHER_FACTS ANTES DO PLAYBOOK QUE CHAMA A ROLE

- name: gather facts
  hosts: all

- name: balancer configuration
  hosts: balancer
  tasks:
    - name: generate configs
      template:
        src: conf.j2
        dest: "/conf/{{ hostvars[item]['ansible_nodename'] }}.conf"
      with_hostnames: nodes


---------------------------------------------------------------------------------
- name: get cluster facts
  hosts: k8s-cluster
  tags:
    - always
  tasks:
    - name:
      setup:
      become: true

- name: deploy HA Proxy
  hosts: kube-master
  become: yes
  roles:
    - { role: ansible-role-haproxy }

  ---------------------------------------------------------------------------------

- Trabalhar com hostvars

- Colocar variáveis no inventário

- QUESTÃO DE FORKS E SERIAL 


##########################################################################################
# TASK
##########################################################################################
#  0 - vimrc do root ?

autocmd FileType yaml setlocal ai ts=2 sw=2 et nu cuc
autocmd FileType yaml colo desert


##########################################################################################
# COPY N PASTE
##########################################################################################
#

v - selecionar
y - copiar
p - cplar

##########################################################################################
# TASK 2 - Instalação ansible e Configuração Inventário (dev,test,prod,balancers)
##########################################################################################

# ansible.cfg

[defaults]
inventory = inventory
remote_user = devops

[privilege_escalation]
become = true
become_user = root
become_method = sudo
become_ask_pass = false

# inventory

[dev]
servera

[test]
serverb

[prod]
serverc

[balancer]
serverd

[web:children]
dev
test



##########################################################################################
# TASK 2 - Script adhoc configura repositórios
##########################################################################################
#

#!/bin/bash 

ansible all -m yum_repository -a "name=elasticsearch-7.x \
                                  state=present \
                                  description='Elasticsearch repository for 7.x packages' \ 
                                  baseurl=https://artifacts.elastic.co/packages/7.x/yum \
                                  gpgcheck=true \
                                  gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch"



##########################################################################################
# TASK 3 - Criação play etc/issue de acordo com ambiente
##########################################################################################
# Development
# Test
# Production
# Balancers

--- 
- name: Configure /etc/issue 
  hosts: all
  become: true 
  tasks: 

  - name: Configure etc/issue at dev 
    copy:
      dest: /etc/issue 
      content: "Development Environment"
    when: inventory_hostname in groups['dev']

  - name: Configure etc/issue at test 
    copy:
      dest: /etc/issue 
      content: "Test Environment"
    when: inventory_hostname in groups['test']

  - name: Configure etc/issue at prod 
    copy:
      dest: /etc/issue 
      content: "Production Environment"
    when: inventory_hostname in groups['prod']


# teste

ansible dev,test,prod -a 'cat /etc/issue'

##########################################################################################
# TASK 4 - Play motd "You are at server FQDN with ip IPV4
##########################################################################################
# 
# 

$ cat motd.j2

Welcome to host {{ansible_hostname}} with ip {{ ansible_default_ipv4.address }}


--- 
- name: Configure /etc/motd 
  hosts: all
  become: true 
  tasks: 

  - name: Configure etc/issue at dev 
    template:
      scr: motd.j2
      dest: /etc/motd
      owner: root
      group: root
      mode: 0644

- teste

ansible all -a 'cat /etc/motd'

##########################################################################################
# TASK 5 - Criação arquivo vault
##########################################################################################
# - cria arquivo secret 
# - cria arquivo com a senha 
 
ansible-vault create secret.yml 
- pass : redpass
- pass : redpass

user_password: redhat 


echo redpass > passwd 


--vault_password_file 

ou

[defaults]
vault_password_file = ./passwd_file 


##########################################################################################
# TASK 6 - Criação de um role do zero 
##########################################################################################
# - instala httpd,firewalld, configura, habilita, com notify
# 

TIVE PROBLEMAS COM AS ASPAS ANTES DOS COLCHETES 

content: "Welcome to server {{ ansible_fqdn }}"

name: "{{ item }}"


---
# tasks/main.yml

- name: Install packages 
  yum:
    name:
      - httpd
      - firewalld
    state: present

- name: Start Services
  service:
    name: "{{ item }}"
    state: started
    enabled: true 
  loop:
    - httpd
    - firewalld 

- name: Configure firewalld 
  firewalld:
    service: http
    state: enabled
    immediate: true
    permanent: true
  notify: restart firewalld 

- name: Create content
  copy: 
    dest: /var/www/html/index.html
    content: "Welcome to server {{ ansible_fqdn }}"
  notify: restart httpd


---
# handlers/main.yml

- name: restart firewalld
  service:
    name: firewalld
    state: restarted

- name: restart httpd
  service:
    name: httpd
    state: restarted


---
# playbook.yml
- name: Config web servers 
  hosts: web
  become: true 
  roles:
    - webserver 



##########################################################################################
# TASK 7 - Criação /etc/hosts com template , todos hosts do inventário , mas só copia pra um deles

##########################################################################################
#   ip fqdn hostname 


 - A GRANDE PEGADINHA AQUI É QUE O host NO ARQUIVO JINJA NA PODE TER A ASPAS SIMPLES E AS DEMAIS VARIÁVEIS SIM
 - como colocar as informações que irão aparecer ? RESPOSTA: SÓ COLOCAR NORMAL

$ cat hosts.j2

{% for host in groups[all] %}
{{ hostvars[host]['ansible_default_ipv4']['address']}} {{ hostvars[host]['ansible_fqdn'] }} {{ hostvars[host]['ansible_hostname'] }}
{% endfor %}

$ cat hosts.yml

--- 
- name: Gera arquivo hosts 
  hosts: all 
  gather_facts: true 
  tasks:

    - name: Gera template hosts 
      template:
        src: hosts2.j2 
        dest: /etc/hosts2
        mode: 0644
        owner: root 
        group: root 
      when: inventory_hostname in groups['dev']



##########################################################################################
# TASK 8 - Configurar um requirements na pasta roles e instalar mudando o nome 
##########################################################################################
# roles: 
# - haproxy
# - phpinfo -

[user@host project]$ cat roles/requirements.yml


# from Ansible Galaxy, using the latest version
- src: geerlingguy.redis

# from Ansible Galaxy, overriding the name and using a specific version
- src: geerlingguy.redis
  version: "1.5.0"
  name: redis_prod

# from any Git-based repository, using HTTPS
- src: https://gitlab.com/guardianproject-ops/ansible-nginx-acme.git
  scm: git
  version: 56e00a54
  name: nginx-acme

# from any Git-based repository, using SSH
- src: git@gitlab.com:guardianproject-ops/ansible-nginx-acme.git
  scm: git
  version: master
  name: nginx-acme-ssh

# from a role tar ball, given a URL;
#   supports 'http', 'https', or 'file' protocols
- src: file:///opt/local/roles/myrole.tar
  name: myrole






# from Ansible Galaxy, overriding the name and using a specific version
- src: geerlingguy.haproxy
  name: load_balancer 



ansible-galaxy install -r roles/requirements.yml -p roles 


##########################################################################################
# TASK - 9 - Configurar um playbook para usar as roles de haproxy e phpinfo 
##########################################################################################
# - dava erro na interface eth0 não tinha uma propriedade blablabla 
# - instala o haproxy no node [balancers] curl deve retornar cada hora um ip 

ansible-galaxy search 'redis' --platforms EL

ansible-galaxy install geerlingguy.haproxy --ignore-certs --platforms EL


##########################################################################################
# TASK 10 - Instalar rhel-system-roles e utilizar a de timesync 
##########################################################################################
#  - deve mudar os server (IP)
#  - manter o provider 
#  - com iburst 

yum install rhel-system-roles

ansible-galaxy list 

ansible all -a 'timedatectl' 

ansible all -a 'chronyc sources -v' 


---
# play_timesync.yml
- name: Time Synchronization Play
  hosts: servers
  vars:
    timesync_ntp_servers:
      - hostname: 172.25.254.254
        iburst: yes

  roles:
    - rhel-system-roles.timesync


ansible all -a 'timedatectl' 

ansible all -a 'chronyc sources -v' 


##########################################################################################
# TASK 11 - Playbook para criar lv 
##########################################################################################
# - dá o nome do vg 
# - se já existir o vg msg : O vg já existe 
#
# - Se não tiver espaço suficiente : Não tem espaço suficiente 
# - Então cria com espaço XX

Create a logical volume with specific size, if the volume group doesn't have enough size output a message and use another size. ( LV name = dbdata and size 2.5G if VG databin have sufficient size or fall back to 1G )

If volume group doesn't exist exit with output message ( if VG databin not there, we skip )

If logical volume created format and mount to disk ( if dbdata LV created, format with ext4 and mount to /db )


DICA!!! :

ansible dev -m setup -a 'filter=ansible_lvm'

ansible dev -a 'vgs'

ansible dev -a 'vgdisplay databin'

ansible dev -a 'vgdisplay /dev/myvg/dbdata'




---
- name: Setup LVM
  hosts: all
  tasks:

    - name: Check for databin VG
      fail:
        msg: VG databin does not exist
      when:
        - ansible_lvm['vgs']['databin'] is not defined

    - name: Create LVM and Filesystem and Mount
      block:

        - name: Check for databin VG Size of 2500MiB
          fail:
            msg: Could not create LV with 2500MiB size
          when:
            - ansible_lvm['vgs']['databin']['size_g'] < "2.50"
      
        - name: Create 2500MiB LVM on databin VG
          lvol:
            lv: dbdata
            vg: databin
            size: "2532"
      
      rescue:
      
        - name: Fall back and Create 1024MiB LVM on databin VG
          lvol:
            size: "1024"
            vg: databin
            lv: dbdata
      
      always:
      
        - name: Set Filesystem
          filesystem:
            dev: /dev/databin/dbdata
            fstype: ext4

        - name: Mount LV 
          mount:
            path: /db
            src: /dev/databin/dbdata
            fstype: ext4
            state: mounted


IMPORTANTE!!! :

is not defined 

ansible_lvm['vgs']['databin']['size_g'] < "2.50"

JÁ MONTA SE mounted OU SÓ COLOCA NA FSTAB present 


##########################################################################################
# TASK 12 - Reencrypt
##########################################################################################
# - reencriptar o secret criado 




##########################################################################################
# TASK 13 - Playbook que pega um arquivo com lista de usuários 
##########################################################################################
# - case X instala no grupo dev 
# - case Y instala no grupo prod 
# - utilizar arquivo vault 
# - utiliza arquivo vars
# - utiliza arquivo senha . Como colocar no play ?




##########################################################################################
# TASK 14 - Criar o /data
##########################################################################################
      # - link com /var/www/html/devweb 
      # - permissão tipo rwx com set UID 
      # - dependendo do usuário vai retornar um valor 
      # - tava dando erro de permissão 


---
- name: Create link /data for webserver 
  hosts: dev
  become: true
  tasks:

    - name: Create link 
      file:
        src: /data
        dest: /var/www/html/data
        state: link
        mode: u=rwx,g=rx,o=---,g+s




##########################################################################################
# TASK 1 : Ansible Installation and Configuration
##########################################################################################
# Install ansible package on the control node (including any dependencies) and configure the following:

#     Create a regular user automation with the password of devops. Use this user for all sample exam tasks and playbooks, unless you are working on the task #2 that requires creating the automation user on inventory hosts. You have root access to all five servers.
#     All playbooks and other Ansible configuration that you create for this sample exam should be stored in /home/automation/plays.

# Create a configuration file /home/automation/plays/ansible.cfg to meet the following requirements:

#     The roles path should include /home/automation/plays/roles, as well as any other path that may be required for the course of the sample exam.
#     The inventory file path is /home/automation/plays/inventory.
#     Privilege escallation is disabled by default.
#     Ansible should be able to manage 10 hosts at a single time.
#     Ansible should connect to all managed nodes using the automation user.

# Create an inventory file /home/automation/plays/inventory with the following:

#     ansible2.hl.local is a member of the proxy host group.
#     ansible3.hl.local is a member of the webservers host group.
#     ansible4.hl.local is a member of the webservers host group.
#     ansible5.hl.local is a member of the database host group.



##########################################################################################
TASK 2: Ad-Hoc Commands
##########################################################################################
# Generate an SSH keypair on the control node. You can perform this step manually.

# Write a script /home/automation/plays/adhoc that uses Ansible ad-hoc commands to achieve the following:

#     User automation is created on all inventory hosts (not the control node).
#     SSH key (that you generated) is copied to all inventory hosts for the automation user and stored in /home/automation/.ssh/authorized_keys.
#     The automation user is allowed to elevate privileges on all inventory hosts without having to provide a password.

# After running the adhoc script on the control node as the automation user, you should be able to SSH into all inventory hosts using the automation user without password, as well as a run all privileged commands.




chmod +x /home/automation/plays/adhoc

cat /home/automation/plays/adhoc

#!/bin/bash

ansible all -u root -m user -a 'name=automation shell=/bin/bash'
ansible all -u root -m authorized_key -a "user='automation' state='present' key='{{ lookup('file','/home/automation/.ssh/id_rsa.pub')}}'"
ansible all -u root -m copy -a "dest=/etc/sudoers.d/automation content='automation ALL = (ALL) NOPASSWD: ALL'"




##########################################################################################
TASK 3: File Content
##########################################################################################
# Create a playbook /home/automation/plays/motd.yml that runs on all inventory hosts and does the following:

#     The playbook should replace any existing content of /etc/motd with text. Text depends on the host group.
#     On hosts in the proxy host group the line should be “Welcome to HAProxy server”.
#     On hosts in the webservers host group the line should be “Welcome to Apache server”.
#     On hosts in the database host group the line should be “Welcome to MySQL server”.


---
- name: File /etc/motd Content 
  hosts: localhost
  tasks:
    - name: Config motd
      copy: 
        content: "Welcome to HAProxy server"
        dest: /etc/motd
      when: inventory_hostname in groups['proxy']

    - name: Config motd
      copy: 
        content: "Welcome to Apache server"
        dest: /etc/motd
      when: inventory_hostname in groups['webservers']

    - name: Config motd
      copy: 
        content: "Welcome to MySQL server"
        dest: /etc/motd
      when: inventory_hostname in groups['database']


##########################################################################################
TASK 4: Configure SSH Server
##########################################################################################
# Create a playbook /home/automation/plays/sshd.yml that runs on all inventory hosts and configures SSHD daemon as follows:

#     banner is set to /etc/motd
#     X11Forwarding is disabled
#     MaxAuthTries is set to 3

---
- name: Configure sshd
  hosts: all
  tasks: 

    - name: Configure sshd banner 
      lineinfile:
        line: "Banner /etc/motd"
        regexp: "^#Banner"
        dest: /etc/ssh/sshd_config
      notify: restart sshd 

    - name: Configure sshd banner 
      lineinfile:
        line: "X11Forwarding no"
        regexp: "^X11Forwarding"
        dest: /etc/ssh/sshd_config
      notify: restart sshd 

    - name: Configure sshd banner 
      lineinfile:
        line: "MaxAuthTries 3"
        regexp: "^#MaxAuthTries"
        dest: /etc/ssh/sshd_config
      notify: restart sshd 

  handlers:

    - name: restart sshd 
      service:
        name: sshd
        state: restarted

##########################################################################################
  # TASK 5: Ansible Vault
##########################################################################################
# Create Ansible vault file /home/automation/plays/secret.yml. Encryption/decryption password is devops.
# Add the following variables to the vault:
#     user_password with value of devops
#     database_password with value of devops
# Store Ansible vault password in the file /home/automation/plays/vault_key.


[defaults]
  vault_password_file = ~/.vault_pass


Create Ansible vault file /home/automation/plays/secret.yml. Encryption/decryption password is devops.

Add the following variables to the vault:

    user_password with value of devops
    database_password with value of devops

Store Ansible vault password in the file /home/automation/plays/vault_key.

# Solução ---------------------------------------------

ansible-vault create secret.yml
New Vault password: 
Confirm New Vault password: 

user_password: devops
database_password: devops

echo "devops" > /home/automation/plays/vault_key


##########################################################################################
# TASK 6: Users and Groups
##########################################################################################
# You have been provided with the list of users below.
# Use /home/automation/plays/vars/user_list.yml file to save this content.

---
users:
  - username: alice
    uid: 1201
  - username: vincent
    uid: 1202
  - username: sandy
    uid: 2201
  - username: patrick
    uid: 2202

# Create a playbook /home/automation/plays/users.yml that uses the vault file /home/automation/plays/secret.yml to achieve the following:
#     Users whose user ID starts with 1 should be created on servers in the webservers host group. User password should be used from the user_password variable.
#     Users whose user ID starts with 2 should be created on servers in the database host group. User password should be used from the user_password variable.
#     All users should be members of a supplementary group wheel.
#     Shell should be set to /bin/bash for all users.
#     Account passwords should use the SHA512 hash format.
#     Each user should have an SSH key uploaded (use the SSH key that you created previously, see task #2).
# After running the playbook, users should be able to SSH into their respective servers without passwords.

--- 
- name: Configure users 
  hosts: all
  vars_files: 
    - /home/automation/plays/secret.yml
    - /home/automation/plays/vars/user_list.yml
  tasks:

    - name: Create users whose ID starts with 1
      user:
        name: "{{ item.username }}"
        groups: wheel
        password: "{{ user_password | password_hash('sha512') }}"
        shell: /bin/bash 
        ssh_key_file: /home/devops/.ssh/id_rsa
      loop: "{{ users  }}"
      when: inventory_hostname in groups['webservers'] and (item.uid | string)[0] | int == 1

    - name: Create users whose ID starts with 1
      user:
        name: "{{ item.username }}"
        groups: wheel
        password: "{{ user_password | password_hash('sha512') }}"
        shell: /bin/bash 
        ssh_key_file: /home/devops/.ssh/id_rsa
      loop: "{{ users  }}"
      when: inventory_hostname in groups['database'] and (item.uid | string)[0] | int == 2


##########################################################################################
TASK 7: Scheduled Tasks
##########################################################################################
# Create a playbook /home/automation/plays/regular_tasks.yml that runs on servers in the proxy host group and does the following:

#     A root crontab record is created that runs every hour.
#     The cron job appends the file /var/log/time.log with the output from the date command.

---
- name: Crontab for root 
  hosts: proxy
  tasks:

    - name: Create crontab 
      cron:
        user: root 
        name: Append date 
        minute: 0
        job: date >> /var/log/time.log 


##########################################################################################
TASK 8: Software Repositories
##########################################################################################
# Create a playbook /home/automation/plays/repository.yml that runs on servers in the database host group and does the following:

#     A YUM repository file is created.
#     The name of the repository is mysql80-community.
#     The description of the repository is “MySQL 8.0 YUM Repo”.
#     Repository baseurl is http://repo.mysql.com/yum/mysql-8.0-community/el/8/x86_64/.
#     Repository GPG key is at http://repo.mysql.com/RPM-GPG-KEY-mysql.
#     Repository GPG check is enabled.
#     Repository is enabled.


---
- name: Configure repository 
  hosts: database
  tasks:

    - name: Create mysql repository
      yum_repository:
        baseurl: http://repo.mysql.com/yum/mysql-8.0-community/el/8/x86_64/
        gpgkey: http://repo.mysql.com/RPM-GPG-KEY-mysql
        gpgcheck: true
        enabled: true
        name: mysql80-community
        description: MySQL 8.0 YUM Repo
        file: mysql80-community

##########################################################################################
# TASK 9: Create and Work with Roles
##########################################################################################
# Create a role called sample-mysql and store it in /home/automation/plays/roles. The role should satisfy the following requirements:

#     A primary partition number 1 of size 800MB on device /dev/sdb is created.
#     An LVM volume group called vg_database is created that uses the primary partition created above.
#     An LVM logical volume called lv_mysql is created of size 512MB in the volume group vg_database.
#     An XFS filesystem on the logical volume lv_mysql is created.
#     Logical volume lv_mysql is permanently mounted on /mnt/mysql_backups.
#     mysql-community-server package is installed.
#     Firewall is configured to allow all incoming traffic on MySQL port TCP 3306.
#     MySQL root user password should be set from the variable database_password (see task #5).
#     MySQL server should be started and enabled on boot.
#     MySQL server configuration file is generated from the my.cnf.j2 Jinja2 template with the following content:

[mysqld]
bind_address = {{ ansible_default_ipv4.address }}
skip_name_resolve
datadir=/var/lib/mysql
socket=/var/lib/mysql/mysql.sock

symbolic-links=0
sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES 

[mysqld_safe]
log-error=/var/log/mysqld.log
pid-file=/var/run/mysqld/mysqld.pid

# Create a playbook /home/automation/plays/mysql.yml that uses the role and runs on hosts in the database host group.

---
- name: Create a partition
  command: parted --script mklabel gpt mkpart primary 1Mib 800MB 

- name: Create vg_database 
  lvg:
    vg: vg_database
    pvs: /dev/vdb1

- name: Create lg 
  lvol:
    vg: vg_database
    lv: lv_mysql
    size: 512m

- name: Create filesystem
  filesystem:
    dev: /dev/vg_database/lv_mysql
    fstype: xfs

- name: Mount filesystem
  mount:
    fstype: xfs
    state: mounted
    opts: noatime
    path: /mnt/mysql_backups
    src: /dev/vg_database/lv_mysql

- name: Install mysql package
  yum:
    name: mysql-community-server
    state: present

- name: Install firewalld
  yum:
    name: firewalld
    state: present

- name: Start firewalld
  service:
    name: firewalld
    state: started
    enabled: true

- name: Enable ports on firewalld 
  firewalld:
    ports: 3306/tcp
    state: enabled
    immediate: true
    permanent: true 

- name: Change mysql root passwd
  mysql_user:
    name: root
    password: "{{ database_password }}"

- name: Start mysqld  
  service:
    name: mysqld
    state: started
    enabled: true

- name: Template a file to /etc/files.conf
  template:
    src: my.cnf.j2
    dest: /etc/mysql/my.cnf
    owner: root
    group: root
    mode: 0644
  notify: restart mysqld

---
- name: Play 
  hosts: database
  vars_files:
    - secret.yml
  roles:
    - sample-mysql


##########################################################################################
# TASK 10: Create and Work with Roles (Some More)
##########################################################################################
# Create a role called sample-apache and store it in /home/automation/plays/roles. The role should satisfy the following requirements:

#     The httpd, mod_ssl and php packages are installed. Apache service is running and enabled on boot.
#     Firewall is configured to allow all incoming traffic on HTTP port TCP 80 and HTTPS port TCP 443.
#     Apache service should be restarted every time the file /var/www/html/index.html is modified.
#     A Jinja2 template file index.html.j2 is used to create the file /var/www/html/index.html with the following content:

The address of the server is: IPV4ADDRESS

#IPV4ADDRESS is the IP address of the managed node.
# Create a playbook /home/automation/plays/apache.yml that uses the role and runs on hosts in the webservers host group.


- name: Install packages
  yum:
    name: "{{ item }}"
    state: present
  loop:
    - httpd
    - mod_ssl
    - php

- name: firewalld configuration
  firewalld:
    service: "{{ item }}"
    permanent: yes
    immediate: yes
    state: enabled
  notify: reload firewalld
  loop:
    - https
    - http

- name: Start httpd
  service:
    name: httpd
    state: started
    enabled: true

- name: Template index.html
  template:
    src: index.html.j2
    dest: /var/www/html/index.html
  notify: restart httpd 



##########################################################################################
# TASK 11: Download Roles From Ansible Galaxy and Use Them
##########################################################################################
# Use Ansible Galaxy to download and install geerlingguy.haproxy role in /home/automation/plays/roles.

# Create a playbook /home/automation/plays/haproxy.yml that runs on servers in the proxy host group and does the following:

#     Use geerlingguy.haproxy role to load balance request between hosts in the webservers host group.
#     Use roundrobin load balancing method.
#     HAProxy backend servers should be configured for HTTP only (port 80).
#     Firewall is configured to allow all incoming traffic on port TCP 80.

# If your playbook works, then doing “curl http://ansible2.hl.local/” should return output from the web server (see task #10). Running the command again should return output from the other web server.


# requirements.yml

 - src: https://example.com/rgeerlingguy.haproxy-1.0.1.tar.gz
   name: load-balancer

# comando instalação role

ansible-galaxy install -r requirements.yml -p roles/


# play.yml

---
- name: Configure load-balancer
  hosts: proxy
  vars:
    haproxy_backend_servers:
      - name: app1
        address: 192.168.0.1:80
      - name: app2
        address: 192.168.0.2:80
    haproxy_backend_balance_method: 'roundrobin'

  roles:
    - load-balancer

  tasks: 

    - name: Install firewalld
      yum:
        name: firewalld
        state: present
    
    - name: Start Firewalld
      service:
        name: firewalld
        state: started
        enabled: true

    - name: Allow incoming traffic
      firewalld:
        port: 80/tcp
        state: enabled
        immediate: true
        permanent: true


##########################################################################################
# TASK 12: Security
##########################################################################################
# Create a playbook /home/automation/plays/selinux.yml that runs on hosts in the webservers host group and does the following:
#     Uses the selinux RHEL system role.
#     Enables httpd_can_network_connect SELinux boolean.
#     The change must survive system reboot.

[root@host ~]# yum install rhel-system-roles

---
- name: Configure Dev Web Server
  hosts: webservers
  vars: 
    selinux_booleans:
      - {name: 'httpd_can_network_connect', state: 'on', persistent: 'yes'}
  
    selinux_booleans:
      - name: 'httpd_enable_homedirs'
        state: 'on'
        persistent: 'yes'

  tasks:

    - name: Apply SELinux role

      block:
    
        - include_role:
            name: rhel-system-roles.selinux
    
      rescue:
    
        - name: Check for failure for other reasons than required reboot
          fail:
          when: not selinux_reboot_required

        - name: Restart managed host
          reboot:

        - name: Reapply SELinux role to complete changes
          include_role:
            name: rhel-system-roles.selinux


##########################################################################################
# TASK 13: Use Conditionals to Control Play Execution
##########################################################################################
# Create a playbook /home/automation/plays/sysctl.yml that runs on all inventory hosts and does the following:

#     If a server has more than 2048MB of RAM, then parameter vm.swappiness is set to 10.
#     If a server has less than 2048MB of RAM, then the following error message is displayed:

# Server memory less than 2048MB

---
- name: Verify memory and set sysctl 
  hosts: all
  tasks:

    - name: Set sysctl vm.swappiness if more than 2048MB
      sysctl:
        name: vm.swappiness
        value: '5'
        state: present
      when: ansible_memtotal_mb > 2048

    - name: send message if < 2048
      debug:
        msg: Server memory less than 2048MB
      when: ansible_memtotal_mb < 2048


##########################################################################################
# TASK 14: Use Archiving
##########################################################################################
# Create a playbook /home/automation/plays/archive.yml that runs on hosts in the database host group and does the following:

#     A file /mnt/mysql_backups/database_list.txt is created that contains the following line: dev,test,qa,prod.
#     A gzip archive of the file /mnt/mysql_backups/database_list.txt is created and stored in /mnt/mysql_backups/archive.gz.

- name: use archiving 
  hosts: database 
  tasks:
    - name: Create file 
      lineinfile:
        create: true
        path: /mnt/mysql_backups/database_list.txt
        line: dev,test,qa,prod
    
    - name: archiving file 
      archive:
        dest: /mnt/mysql_backups/archive.gz
        path: mnt/mysql_backups/database_list.txt


##########################################################################################
# TASK 15: Work with Ansible Facts
##########################################################################################
# Create a playbook /home/automation/plays/facts.yml that runs on hosts in the database host group and does the following:
#     A custom Ansible fact server_role=mysql is created that can be retrieved from ansible_local.custom.sample_exam when using Ansible setup module.

- name: use custom variables 
  hosts: database 
  tasks:

    - name: Ensure directory exists 
      file:
         path: /etc/ansible/facts.d
         state: directory
         recurse: true 

    - name: Create file 
      copy:
        dest: /etc/ansible/facts.d/custom.fact
        content: "[sample_exam]\nserver_role=mysql"

    - name: Verifica
      command: cat /etc/ansible/facts.d/custom.fact   
    
    - name: Retrieve 
      debug:
        var: ansible_local.custom.sample_exam

##########################################################################################
# TASK 16: Software Packages
##########################################################################################
# Create a playbook /home/automation/plays/packages.yml that runs on all inventory hosts and does the following:
#     Installs tcpdump and mailx packages on hosts in the proxy host groups.
#     Installs lsof and mailx packages on hosts in the database host groups.


---
- name: Install packages
  hosts: all
  become: true
  tasks:

    - name: Install packages on proxy servers
      yum:
        name:
          - tcpdump
          - mailx
        state: present
      when: inventory_hostname in groups['proxy']

    - name: Install packages on database servers
      yum:
        name:
          - lsof
          - mailx
        state: present
      when: inventory_hostname in groups['database']


##########################################################################################
# TASK 17: Services
##########################################################################################
# Create a playbook /home/automation/plays/target.yml that runs on hosts in the webservers host group and does the following:
#     Sets the default boot target to multi-user.

---
- name: Change default boot target
  hosts: webservers
  tasks:
    - name: Default boot target is multi-user
      file:
        src: /usr/lib/systemd/system/multi-user.target
        dest: /etc/systemd/system/default.target
        state: link

##########################################################################################
# TASK 18. Create and Use Templates to Create Customised Configuration Files
##########################################################################################
# Create a playbook /home/automation/plays/server_list.yml that does the following:

#     Playbook uses a Jinja2 template server_list.j2 to create a file /etc/server_list.txt on hosts in the database host group.
#     The file /etc/server_list.txt is owned by the automation user.
#     File permissions are set to 0600.
#     SELinux file label should be set to net_conf_t.
#     The content of the file is a list of FQDNs of all inventory hosts.

# After running the playbook, the content of the file /etc/server_list.txt should be the following:

# ansible2.hl.local
# ansible3.hl.local
# ansible4.hl.local
# ansible5.hl.local

# Note: if the FQDN of any inventory host changes, re-running the playbook should update the file with the new values.


# server_list.j2

{% for host in groups['all'] %}
{{ hostvars[host]['ansible_fqdn'] }}
{% endfor %}


- name: Create server list
  hosts: all
  tasks:
    - name: Create a list of servers on webservers
      template:
        src: server_list.j2
        dest: /etc/server_list.txt
        owner: devops
        group: devops
        mode: 0600
        setype: net_conf_t
      when: inventory_hostname in groups['web']












